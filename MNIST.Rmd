---
title: "MNIST"
output: html_document
---
#**Chapter 8 K-Nearest Neighbors**
K-nearest neighbor (KNN) is a very simple algorithm in which each observation is predicted based on its “similarity” to other observations. Unlike most methods in this book, KNN is a memory-based algorithm and cannot be summarized by a closed-form model. This means the training samples are required at run-time and predictions are made directly from the sample relationships. Consequently, KNNs are also known as lazy learners (Cunningham and Delany 2007) and can be computationally inefficient. However, KNNs have been successful in a large number of business problems (see, for example, Jiang et al. (2012) and Mccord and Chuah (2011)) and are useful for preprocessing purposes as well (as was discussed in Section 3.3.2).
```{r}
#Do parallel
library(doParallel)
cl <-makePSOCKcluster(5)
registerDoParallel(cl)
# Helper packages
library(dplyr)      # for data wrangling
library(ggplot2)    # for awesome graphics
library(rsample)    # for creating validation splits
library(recipes)    # for feature engineering

# Modeling packages
library(caret)       # for fitting KNN models
# import MNIST training data
mnist <- dslabs::read_mnist()
names(mnist)
## [1] "train" "test"
```


```{r}
```


```{r}
```


```{r}
```

